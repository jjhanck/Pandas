{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaled attributes from the training and testing data are now used for investigating associated weights and support vectors. This is done by using support vector machines (SVMs) on the data and playing with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Pandas to import as a data frame\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(42000, n_iter=3, test_size=0.2, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "\n",
    "# Create training set and cross validation set\n",
    "# Use 50% of data for cross validation\n",
    "# dftrain, dfcross = train_test_split(train, test_size = 0.5)\n",
    "\n",
    "# Set X and y\n",
    "X = train.values\n",
    "y = train['label'].values\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n=num_instances,\n",
    "                         n_iter=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for Null values or duplicate rows...  None found\n"
     ]
    }
   ],
   "source": [
    "# Check data for null values or duplicate rows\n",
    "print \"Checking for Null values or duplicate rows... \", \n",
    "if train.isnull().all().any() or train.duplicated().all(): \n",
    "    print('error in data')\n",
    "else:\n",
    "    print('None found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scl_obj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-050bca0b2959>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mX_train_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscl_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Training\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mX_test_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscl_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scl_obj' is not defined"
     ]
    }
   ],
   "source": [
    "# Run cross validation loop \n",
    "# Set training and testing variable for one iteration\n",
    "for train_indices, test_indices in cv_object: \n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # Training\n",
    "X_test_scaled = scl_obj.transform(X_test) # Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-4296f5943ee7>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-4296f5943ee7>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    y_hat = svm_clf.predict(X_test_scaled)\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Use SVMs on data \n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# Train the model\n",
    "svm_clf = SVC(C=0.5, kernel='linear', degree=3, gamma='auto') # Get object\n",
    "svm_clf.fit(X_train_scaled, y_train # Train object\n",
    "\n",
    "# Get test set precitations\n",
    "y_hat = svm_clf.predict(X_test_scaled)\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training data in half to allow cross validation\n",
    "# The method,train_test_split, used to perform split via randomization\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'svm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9ca851e50ff6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;31m# max_iter=1000)[source]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m nsvc_clf = sk.svm.NuSVC(nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, \n\u001b[0m\u001b[0;32m     14\u001b[0m       \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m       verbose=False, max_iter=1000, decision_function_shape=None, random_state=None)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'svm'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#  default settings sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', \n",
    "            # dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True,\n",
    "            # intercept_scaling=1, class_weight=None, verbose=0, random_state=None, \n",
    "            # max_iter=1000)[source]\n",
    "            \n",
    "nsvc_clf = sk.svm.NuSVC(nu=0.5, kernel='rbf', degree=3, gamma='auto', coef0=0.0, shrinking=True, \n",
    "      probability=False, tol=0.001, cache_size=200, class_weight=None, \n",
    "      verbose=False, max_iter=1000, decision_function_shape=None, random_state=None)\n",
    "\n",
    "nsvc_clf.fit(X_train_scaled, y_train)\n",
    "y_hat = svm_clf.predict(X_test_scaled) \n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "\n",
    "print('')\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d6a2c8e1393d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Number of support vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msvm_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_support_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'svm_clf' is not defined"
     ]
    }
   ],
   "source": [
    "# Number of support vectors\n",
    "svm_clf.n_support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "print(svm_clf.coef_)\n",
    "weights = pd.Series(svm_clf.coef_[0],index=train.columns)\n",
    "\n",
    "sns = weights.plot(color = \"purple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plot above shows a high concentration of vectors in pixels 300-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use SVM now for some different analysis\n",
    "# Take closer look at instances previously chosen as support vectors\n",
    "# Next take closer look at support for the vectors\n",
    "# Determine if indicative of anything\n",
    "\n",
    "# Get rows previously selected as support vectors\n",
    "\n",
    "# Training data dataframe (same indices previously chosen)\n",
    "df_tested_on = train.iloc[train_indices] \n",
    "\n",
    "# Get support vectors from trained model\n",
    "df_support = df_tested_on.loc[svm_clf.support_,:]\n",
    "\n",
    "# Add\n",
    "df_support['label'] = y[svm_clf.support_] \n",
    "# add back in the 'Survived' Column to the pandas dataframe\n",
    "train['label'] = y \n",
    "# also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take closer look at the attributes\n",
    "from pandas.tools.plotting import boxplot\n",
    "\n",
    "# Group original data & support vectors\n",
    "df_grouped_support = df_support.groupby(['label'])\n",
    "df_grouped = train.groupby(['label'])\n",
    "\n",
    "# Plot KDE with different variables\n",
    "vars_to_plot = []\n",
    "for i in range(len(df_grouped)):\n",
    "    if i > 300 and i < 500:\n",
    "        vars_to_plot.append(df_grouped.columns[i])\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    sns = plt.figure(figsize=(10,4))\n",
    "    \n",
    "    # Support vector statistics plotted\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['pixel300','pixel450'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # Original distributions plotted\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['pixel300','pixel450'])\n",
    "    plt.title(v+' (Original)')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
